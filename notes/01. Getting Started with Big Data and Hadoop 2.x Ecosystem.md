# Getting Started with Big Data and Hadoop 2.x Ecosystem

> **Big Data** refers to the data which is **large, fast and complex type of structured, semi-structured, un-structured** data generated from variety of different source, which becomes **difficult to store and analyze using a traditional processing system.**

**Challenges of Big Data**

1. Storage : Distributed Storage System
2. Processing : Massive Parallel Processing Framework (MPP)

## Hadoop 2.x (Distributed Storage and Processing Framework)

> Hadoop is a software framework that allows us to store and process large datasets in parallel and distributed fashion

## Core Components of Hadoop

1. Storage Layer : HDFS (Hadoop Distributed File System)
2. Cluster Manager : YARN (Yet Another Resource Negotiator)
3. Processing Layer : MapReduce

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdBh-mLVF0fNjESG5B6XbkIv6F2dQ0lut-I41B4RFN_wNUzmZilWz51kjHtVojZpOS6DJPl3WtR7pPJhjY6VP8fgOpVZEpfiWN5sNTfAooB1v5xxeinlqeSCHeS2DXbIKDmRcel7Ro7j7jjMuuheP9DJG1U?key=Lcjgu0sLjm8U8i3A_14gRg)

## Daemon Services

1. NameNode
2. SecondaryNameNode
3. DataNode
4. ResourceManager
5. NodeManager

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfDx8NJn_oTiSlf9OKh-qnlKok2EDiHnmbGfV-tTDDoUep6MhNpfL_UBdVnreNw1wfl4KFl6A3vXGWWG1tsjaQDzriUNJ4ZmzkGYA5UqFxNmvoHZtzz4dYm0BauCUnffn7Lqu9ru5gkGoZyXTp7bSg87AA?key=Lcjgu0sLjm8U8i3A_14gRg)

## Master and Slave Architecture

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdmntuVzOBhU2C96L4IZEmcjbPKR4PZfo_U2awBZX7APiKVTJgWUo-8mqEIrAbCdJZb9DnWYA21lfw48rRxDr4_IiJ2lkL7P8uhQC605oOmdRJst0hNYHlPDQq18RFNhN2ahXlHmjnNC73g-7nTKSdZ9jUK?key=Lcjgu0sLjm8U8i3A_14gRg)

## HDFS and Architecture

> HDFS is a **distributed** and **scalable file system** designed to store very large files

**Distributed**

* The files are split into blocks and data will be stored in multiple machine.

**Scalable**

* Increase or decrease the nodes