# Exploring Spark SQL and DatFrame API

> Spark SQL is one of the **module in the Spark eco-system** to query and **analyze structured and semi-structured data.**

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfE1lMuYpHKoU7vVLvmGb1HdYoIo_XZE35acix3746cuVNQ4RLrN7ac3tY8EkVLmhY5a-PyM4V_8PR5ucp-OTzeO-J2Rwa9FXiZO5CjQI2zb-pDJcgoVH7GyiAz-40vZ6Fca4CzhbUfOScQQ1EJDjFcCpU?key=yGW25KMloT80Lch6YWjT9A)

**Important Points**

* Using Spark SQL we can perform Batch Processing and Stream Processing
* We can interact with Spark SQL using SQL, DataFrame and DataSet API (Not supported with Python)

## What is Spark DataFrame

> A DataFrame is a distributed collection of data organized into named columns

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc90AB43RK2Wo86dMFXxYvCXTdipduMWdRzwwtlMm3ww9atEsIvPzy_XB38YEhb0eU1DSJCOFIxG_RyeTm8vIxzG6J6WfssRh6C4S_-_cnyL9tc8C-0MHHmJDkbdSPV2kt2WfwpokZy_JwQGG7cRIo01IJ4?key=yGW25KMloT80Lch6YWjT9A)

* It is similar to tables in relational database, 
* Once a DataFrame is created we can perform different operations (select, filter, grouping, sorting, aggregations, window functions, jo)

**Important Points**

* The primary interface to create DataFrame is via DataFrameReader (C)
* All the methods to create DataFrame is present inside DataFrameReader
* DataFrameReader
  * .csv -> DataFrame
  * .json -> DataFrame
  * .parquet -> DataFrame
  * .orc -> DataFrame
  * .jdbc -> DataFrame

```
dfr = spark.read

```

**Methods present in column object**

```
from pyspark.sql.functions import col
df.select(col("name").methodName())
# col("col_name") -> Column
```



* .alias
* .isin
* .between
* isNull
* isNoNull
* .cast
* .startWith
* .endsWith
* .contains

**Method in DataFrame**

<u>Transformations</u>

* .select
* .filter
* .agg
* .sort
* .groupBy
* .withColumn
* .limit

<u>Action</u>

* .count
* .display
* .printSchema
* .columns

**from pyspark.sql.functio-xdsklaOP	ns import**

* col
* sum
* max
* min
* avg
* count
* when
* lit
