# Getting Started with Apache Spark

> Apache Spark is an **In-memory** cluster framework designed to handle a wide range of big data workloads.

**Application of Apache Spark**

1. Data Integration and ETL
2. Batch Processing
3. Stream Processing
4. Graph Computation
5. Machine Learning Analytics

**Important Points**

* Apache Spark is natively written using Scala

## What is PySpark

> PySpark is the Python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdxrH9bZJ635Qimn2XVv26-Z83-rj6qBTE-eLDpUk7XLAU29uE_zBnVW46JkVLn44-P8G7D20ap7pNY2HzOo6-GV82LZRz0RWbKLQQJhNjGCKMP--0CeowDsAlQsvsCUFIs-5OK6jqNHHEwxvOlOOBQyAHA?key=_he-T4Jq934AhrSZa-Be-g)

* Pyspark communicates with Spark using Py4J API

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcvvVv89D0hw0sk9-3wTHMF4iMCWywWRg2zamF3M5adGT_f9RYiNfkICYTE4sP3qexijYEVZ82qWD5Yrupl2HQCnCNXBmtDj8YndMaeeK7R18VlybRk-bC5bSIrRu9IeMkekPKnraRngyYK_j-iAOsDPcNa?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Interactive Shell

1. Spark Shell (Scala)
2. PySpark Shell (Python)