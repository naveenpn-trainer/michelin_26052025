# Getting Started with Apache Spark

> Apache Spark is an **In-memory** cluster framework designed to handle a wide range of big data workloads.

**Application of Apache Spark**

1. Data Integration and ETL
2. Batch Processing
3. Stream Processing
4. Graph Computation
5. Machine Learning Analytics

**Important Points**

* Apache Spark is <u>natively written using Scala</u>

## What is PySpark

> PySpark is the Python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdxrH9bZJ635Qimn2XVv26-Z83-rj6qBTE-eLDpUk7XLAU29uE_zBnVW46JkVLn44-P8G7D20ap7pNY2HzOo6-GV82LZRz0RWbKLQQJhNjGCKMP--0CeowDsAlQsvsCUFIs-5OK6jqNHHEwxvOlOOBQyAHA?key=_he-T4Jq934AhrSZa-Be-g)

* Pyspark communicates with Spark using Py4J API

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcvvVv89D0hw0sk9-3wTHMF4iMCWywWRg2zamF3M5adGT_f9RYiNfkICYTE4sP3qexijYEVZ82qWD5Yrupl2HQCnCNXBmtDj8YndMaeeK7R18VlybRk-bC5bSIrRu9IeMkekPKnraRngyYK_j-iAOsDPcNa?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Interactive Shell

1. Spark Shell (Scala)
2. PySpark Shell (Python)

**Important Points**

* Two object will be created 
  * SparkContext : sc
  * SparkSession : spark
* Every spark application we launch we will Web UI
* By default cluster manager is local]*]

## Spark API's for Data Processing

1. Low Level API's (RDD's) <mark>Not Recommended</mark>
2. High Level API (DataFrame, DataSet API)

## Building Spark Application

1. Load the data
2. Process the data
3. Write results

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc349DZvywefxfCKkkk3SdT5SamE75hMP5ZIlW7OXr3xarOMzHwWRK1g-ANMmfmhlba9idoi0ew2EuPuEHaNDRaNrZjl0sLG7w1dLuMvVjkU9qp9MvU2PjL30DmFeiukwijqGsV_-UKAMPvDG_huNIPfvc?key=_he-T4Jq934AhrSZa-Be-g)

## RDD's

> RDD's stands for Resilient Distributed Dataset, are the building blocks of any spark application, a fundamental data structure in Apache Spark.

**Partitions**

> RDD's is a collection of objects / data which is partitioned and distributed across nodes in a cluster.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdpkEvRyhZkmJv6XORd6-ua1wUSBu4-kEROEt65gLc9wgpki-hzC2-ko_zo14GXBqelaiCNe0Ck_J0JuNNT4FQlN8sYigE523EAkEv2NGYYMvzV0h1YyD7AopyUWS8ZTyt6cVfxFygB02_aQF1Qn8_V4eyB?key=Dxp7lTxgvspH2ig-I7LuEw)

### RDD Creation

There are two popular ways to create an RDD

1. Create an RDD from Collection

   ```
   L = list(range(1,101))
   rdd = sc.parallelize(L)
   ```

2. Create an RDD from External source

   ```
   rdd = sc.textFile("c:/users.dat")
   ```

   

**Important Points**

* All the methods to create RDD is present inside SparkContext (sc)

### RDD / DataFrame Operation

1. Transformation
2. Actions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXctWMhxv_Hy-Z64hvwOa5J92oUH8m9lDz_VaXvntdjaa-v1fYEzjcLWZDo-u89T82whdPoMMUrYGSKjOB6ATIpsU0Rsob2FPUoXmsTlM3S-Y0ubWNbk3mbfKtneykL_zlC4MSDj9XudN5oiqOcxPZshN20G?key=Dxp7lTxgvspH2ig-I7LuEw)

#### Transformations

* `Transformations creates a new RDD / DataFrame from existing.
* E.g : map(), filter()
* An RDD depending on one or more RDD. This kind of graph that show relationship between RDD's is called **Linear Graph or DAG**

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfe2IUsSblnMd3sWP3hgzBuKBFIz93mi7RfjH0Z_KFV5JijpQ3VOM12txYtHhNxu5UhoyZXjkPwi0G9pbI2PoEddBe3-Tdc1BurooLMddNY6okbSBHhAV6Hd77L_WvxBflkln5kmc3srtDUtBIvFOiavLjk?key=Dxp7lTxgvspH2ig-I7LuEw)

#### Actions

* Actions act of RDD <u>which carry the final computation</u>
* E.g : count(), first(), take(n), .collect(), reduce()

**Revision**

* DataFrameReader (C)
  * .csv
  * .json
  * .parqet
* Parameters for .csv(sep=",", path="", inferSchema=False, header=False)
* Parameters for .json(multiLine)
* 

## DataFrame Operations

* df
  * .printSchema()
  * .limit(n)
  * .display()
  * .columns